# Лабораторная работа №1. Перцептрон

**1. Данные:**
   
   В качестве входных данных к лабораторной работе взят набор данных MNIST - это большая коллекция рукописных цифр. MNIST содержит        коллекцию из  70 000 изображений (60 000 на обучающей и 10 000 на тестовой) 28 x 28  рукописных цифр от 0 до 9. Набор данных уже       разделен на наборы для обучения и тестирования.
   
**2. Реализован однослойный перцептрон без использования библиотек:**
   - В качестве функции активаци выбрана ReLU (Rectified Linear Unit).
   - Реализован прямой проход.
   - Расчитана ошибка на всем наборе данных.
   - Реализован обратный проход, с коррекцией весов нейронной сети.
   - Реализована функция обучения нейронной сети с параметром количества эпох (итераций).
   - Расчитана точность (accuracy) на тестовой и обучающей выборках.
   - Построены графики функции потерь (ошибки) и точности на двух выборках.

**3. Произведен сравнительный анализ результатов с использованием [однослойного перцептрона из sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html).**
     
**4. Произведен сравнительный анализ результатов с использованием [многослойного перцептрона из sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).**

**Обучаем модель градиентным спуском:**

- Берем подмножество данных (Х, у).
- Получаем на этом подмножестве предсказания р=М(Х), где М - выбранная модель (линейная регрессия, нейронная сеть и другие).
- Вычисляем функцию ошибки L(y, p), где у - метка; р - предсказание модели.
- Вычисляем градиент dL/dW, где W - обучаемые параметры (вектор весов).
- Обновляем веса модели: W = W - lr * dL/dW, где lr - learning rate, т.е. скорость обучения или шаг (маленькое число, подбирается вручную).
- Повторяем пункты, перечисленные выше, N раз.

  **Ссылки:**
  
